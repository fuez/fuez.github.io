<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>ceph - Andy's Blog</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="Andy Zhang"><meta name=description content="How to benchmark rbd performance 1 2 3 4  RBD_IMAGE_NAME=&amp;#34;bench1&amp;#34; rbd create --size=10G $RBD_IMAGE_NAME rbd -p replicapool bench $RBD_IMAGE_NAME --io-type write --io-size 8192 --io-threads 256 --io-total 10G --io-pattern seq/rand rbd -p replicapool bench $RBD_IMAGE_NAME --io-type read--io-size 8192 --io-threads 256 --io-total 10G --io-pattern seq/rand   Final way to disconnect a POD reference to ceph volume is by restarting the k8s node hosting that POD LESSON: The first cephmon node used in ceph storage class monitors is critical When that server is down or too busy, all connections to ceph cluster from k8s POD will suffer."><meta name=keywords content="linux,SRE,software"><meta name=generator content="Hugo 0.60.1 with theme even"><link rel=canonical href=http://localhost:1313/post/tips/ceph/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/dist/even.c2a46f00.min.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="ceph"><meta property="og:description" content="How to benchmark rbd performance 1 2 3 4  RBD_IMAGE_NAME=&#34;bench1&#34; rbd create --size=10G $RBD_IMAGE_NAME rbd -p replicapool bench $RBD_IMAGE_NAME --io-type write --io-size 8192 --io-threads 256 --io-total 10G --io-pattern seq/rand rbd -p replicapool bench $RBD_IMAGE_NAME --io-type read--io-size 8192 --io-threads 256 --io-total 10G --io-pattern seq/rand   Final way to disconnect a POD reference to ceph volume is by restarting the k8s node hosting that POD LESSON: The first cephmon node used in ceph storage class monitors is critical When that server is down or too busy, all connections to ceph cluster from k8s POD will suffer."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/post/tips/ceph/"><meta property="article:published_time" content="2018-07-06T16:52:44+08:00"><meta property="article:modified_time" content="2019-08-12T18:01:23+08:00"><meta itemprop=name content="ceph"><meta itemprop=description content="How to benchmark rbd performance 1 2 3 4  RBD_IMAGE_NAME=&#34;bench1&#34; rbd create --size=10G $RBD_IMAGE_NAME rbd -p replicapool bench $RBD_IMAGE_NAME --io-type write --io-size 8192 --io-threads 256 --io-total 10G --io-pattern seq/rand rbd -p replicapool bench $RBD_IMAGE_NAME --io-type read--io-size 8192 --io-threads 256 --io-total 10G --io-pattern seq/rand   Final way to disconnect a POD reference to ceph volume is by restarting the k8s node hosting that POD LESSON: The first cephmon node used in ceph storage class monitors is critical When that server is down or too busy, all connections to ceph cluster from k8s POD will suffer."><meta itemprop=datePublished content="2018-07-06T16:52:44+08:00"><meta itemprop=dateModified content="2019-08-12T18:01:23+08:00"><meta itemprop=wordCount content="540"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="ceph"><meta name=twitter:description content="How to benchmark rbd performance 1 2 3 4  RBD_IMAGE_NAME=&#34;bench1&#34; rbd create --size=10G $RBD_IMAGE_NAME rbd -p replicapool bench $RBD_IMAGE_NAME --io-type write --io-size 8192 --io-threads 256 --io-total 10G --io-pattern seq/rand rbd -p replicapool bench $RBD_IMAGE_NAME --io-type read--io-size 8192 --io-threads 256 --io-total 10G --io-pattern seq/rand   Final way to disconnect a POD reference to ceph volume is by restarting the k8s node hosting that POD LESSON: The first cephmon node used in ceph storage class monitors is critical When that server is down or too busy, all connections to ceph cluster from k8s POD will suffer."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Andy's Blog</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Andy's Blog</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>ceph</h1><div class=post-meta><span class=post-time>2018-07-06</span><div class=post-category><a href=/categories/tip/>tip</a></div><span class=more-meta>540 words</span>
<span class=more-meta>3 mins read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><a href=#how-to-benchmark-rbd-performancehttpsedenmalmoepost2017cephrbdbenchcommands>How to benchmark rbd performance</a></li><li><a href=#final-way-to-disconnect-a-pod-reference-to-ceph-volume-is-by-restarting-the-k8s-node-hosting-that-pod>Final way to disconnect a POD reference to ceph volume is by restarting the k8s node hosting that POD</a></li><li><a href=#lesson-the-first-cephmon-node-used-in-ceph-storage-class-monitors-is-critical>LESSON: The first cephmon node used in ceph storage class monitors is critical</a></li><li><a href=#ceph-hot-commandshttpswwwcnblogscomboshenhzbp6782303html>ceph hot commands</a></li><li><a href=#how-to-manage-local-mapped-rbd-images>How to manage local mapped rbd images?</a></li><li><a href=#rookhttpsrookgithubiodocsrookv09cephquickstarthtml-is-a-good-cloud-native-storage-platform-that-integrates-with-ceph-and-is-worth-for-a-try>rook is a good cloud native storage platform that integrates with ceph, and is worth for a try</a></li><li><a href=#ceph-pg-dump-shows-probably-the-most-overwheelming-information-and-use-ceph-pg-pgid-query-to-show-detail-about-a-specific-pg>ceph pg dump shows probably the most overwheelming information, and use ceph pg <pgid>query to show detail about a specific pg</a></li><li><a href=#ceph-df-to-show-ceph-storage-status>ceph df to show ceph storage status</a></li><li><a href=#watch-ceph-detail-message-by-ceph-s-watchdebug>Watch ceph detail message by ceph -s --watch-debug</a></li><li><a href=#ceph-rolling-upgrades-with-ansiblehttpscephcomgeencategoriecephrollingupgradeswithansible>Ceph rolling upgrades with ansible</a></li><li><a href=#install-backy2-on-centos>Install backy2 on centos</a></li><li><a href=#ts-infocephcreatekeyscannot-get-or-create-admin-key-permission-denied-when-deploying-cluster-with-cephansible>TS: INFO:ceph-create-keys:Cannot get or create admin key, permission denied when deploying cluster with ceph-ansible</a></li><li><a href=#backup-tool-backy2httpsgithubcomlabiebacky2>Backup tool backy2</a></li><li><a href=#how-to-resize-pv-on-k8shttpskubernetesioblog20180712resizingpersistentvolumesusingkubernetes>How to resize pv on k8s</a></li><li><a href=#how-to-resize-a-volumehttpdocscephcomdocsmimicrbdradosrbdcmds>How to resize a volume</a></li><li><a href=#how-to-mount-a-volume-to-localhttpsblogprogramsterorgcephdeployandmountablockdevice>How to mount a volume to local?</a></li><li><a href=#how-to-list-rbd-and-pools-and-show-size-for-a-rbd-image>How to list rbd and pools, and show size for a rbd image?</a></li></ul></nav></div></div><div class=post-content><h2 id=how-to-benchmark-rbd-performancehttpsedenmalmoepost2017cephrbdbenchcommands><a href=https://edenmal.moe/post/2017/Ceph-rbd-bench-Commands/>How to benchmark rbd performance</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sh data-lang=sh><span class=nv>RBD_IMAGE_NAME</span><span class=o>=</span><span class=s2>&#34;bench1&#34;</span>
rbd create --size<span class=o>=</span>10G <span class=nv>$RBD_IMAGE_NAME</span>
rbd -p replicapool bench <span class=nv>$RBD_IMAGE_NAME</span> --io-type write --io-size <span class=m>8192</span> --io-threads <span class=m>256</span> --io-total 10G --io-pattern seq/rand
rbd -p replicapool bench <span class=nv>$RBD_IMAGE_NAME</span> --io-type read--io-size <span class=m>8192</span> --io-threads <span class=m>256</span> --io-total 10G --io-pattern seq/rand
</code></pre></td></tr></table></div></div><h2 id=final-way-to-disconnect-a-pod-reference-to-ceph-volume-is-by-restarting-the-k8s-node-hosting-that-pod>Final way to disconnect a POD reference to ceph volume is by restarting the k8s node hosting that POD</h2><h2 id=lesson-the-first-cephmon-node-used-in-ceph-storage-class-monitors-is-critical>LESSON: The first cephmon node used in ceph storage class <code>monitors</code> is critical</h2><p>When that server is down or too busy, all connections to ceph cluster from k8s POD will suffer. One workaround might be to define an external service for monitors, but at least k8s service solution does not work.</p><h2 id=ceph-hot-commandshttpswwwcnblogscomboshenhzbp6782303html><a href=https://www.cnblogs.com/boshen-hzb/p/6782303.html>ceph hot commands</a></h2><h2 id=how-to-manage-local-mapped-rbd-images>How to manage local mapped rbd images?</h2><pre><code>- to show all mapped rbd: `rbd showmapped`
- to show one mapped rbd status: `rbd status &lt;pool&gt;/&lt;image&gt; `
- to remove one rbd mapping: `rbd rm &lt;pool&gt;/&lt;image&gt; `
- to blacklist a client from accessing os: `ceph osd blacklist add &lt;client inf&gt;`. Please note this command will blacklist all connections from this client, thus will lead to other rbd mapping failure
- to remove a client from a blacklist: `ceph osd blacklist rm &lt;client info&gt;`
- to clear blacklist: `ceph osd blacklist clear`
</code></pre><h2 id=rookhttpsrookgithubiodocsrookv09cephquickstarthtml-is-a-good-cloud-native-storage-platform-that-integrates-with-ceph-and-is-worth-for-a-try><a href=https://rook.github.io/docs/rook/v0.9/ceph-quickstart.html>rook</a> is a good cloud native storage platform that integrates with <em>ceph</em>, and is worth for a try</h2><h2 id=ceph-pg-dump-shows-probably-the-most-overwheelming-information-and-use-ceph-pg-pgid-query-to-show-detail-about-a-specific-pg><code>ceph pg dump</code> shows probably the most overwheelming information, and use <code>ceph pg &lt;pgid> query</code> to show detail about a specific pg</h2><h2 id=ceph-df-to-show-ceph-storage-status><code>ceph df</code> to show ceph storage status</h2><h2 id=watch-ceph-detail-message-by-ceph-s-watchdebug>Watch ceph detail message by <code>ceph -s --watch-debug</code></h2><h2 id=ceph-rolling-upgrades-with-ansiblehttpscephcomgeencategoriecephrollingupgradeswithansible><a href=https://ceph.com/geen-categorie/ceph-rolling-upgrades-with-ansible/>Ceph rolling upgrades with ansible</a></h2><h2 id=install-backy2-on-centos>Install backy2 on centos</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sh data-lang=sh><span class=c1># install gcc, python36, python36-devel and pip3</span>
yum install -y gcc python36 python36-devel python36-pip
pip install backy2

<span class=c1># create a cfg file in the first default path: /etc/backy.cfg, override the following configurations</span>

curl https://raw.githubusercontent.com/wamdam/backy2/af96dfe9d8bae23f96697395c8bd42a6c8aeff91/etc/backy.cfg -O /etc/backy.cfg

</code></pre></td></tr></table></div></div><h2 id=ts-infocephcreatekeyscannot-get-or-create-admin-key-permission-denied-when-deploying-cluster-with-cephansible>TS: <code>INFO:ceph-create-keys:Cannot get or create admin key, permission denied</code> when deploying cluster with <code>ceph-ansible</code></h2><p>This error is due to wrong <code>monitor_interface</code>, should set it to the one in <code>public_network</code> configuration.</p><h2 id=backup-tool-backy2httpsgithubcomlabiebacky2>Backup tool <a href=https://github.com/labie/backy2>backy2</a></h2><h2 id=how-to-resize-pv-on-k8shttpskubernetesioblog20180712resizingpersistentvolumesusingkubernetes><a href=https://kubernetes.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/>How to resize pv on k8s</a></h2><h2 id=how-to-resize-a-volumehttpdocscephcomdocsmimicrbdradosrbdcmds><a href=http://docs.ceph.com/docs/mimic/rbd/rados-rbd-cmds/>How to resize a volume</a></h2><p>The first step is to first mount the target volume locally (make sure to stop other mounts including from k8s POD).
Then use command like <code>rbd resize --size 3072 kube/kubernetes-dynamic-pvc-17b36f46-3682-11e9-82dc-8ad82d31028c</code> to resize volume to 3G.
And then grow file system for that volume, like this for xfs: <code>xfs_growfs /mnt/somepv</code>, but that make sure there is no active connection to this volume.</p><h2 id=how-to-mount-a-volume-to-localhttpsblogprogramsterorgcephdeployandmountablockdevice><a href=https://blog.programster.org/ceph-deploy-and-mount-a-block-device>How to mount a volume to local?</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sh data-lang=sh><span class=c1># map a rbd volume called kubernetes-dynamic-pvc-17b36f46-3682-11e9-82dc-8ad82d31028c in the pool kube</span>
<span class=c1># against monitor node host called ceph2</span>
rbd map kube/kubernetes-dynamic-pvc-17b36f46-3682-11e9-82dc-8ad82d31028c --name client.admin -m ceph2  <span class=c1># this command map remote rbd volume to local device file</span>
mkdir /mnt/somepv
mount /dev/rbd/kube/kubernetes-dynamic-pvc-17b36f46-3682-11e9-82dc-8ad82d31028c  /mnt/somepv
</code></pre></td></tr></table></div></div><p>To watch rbd status to check who is connecting to the rbd: <code>rbd status kube/kubernetes-dynamic-pvc-17b36f46-3682-11e9-82dc-8ad82d31028c</code></p><p>To unmount:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sh data-lang=sh>umount /mnt/somepv
rbd unmap kube/kubernetes-dynamic-pvc-17b36f46-3682-11e9-82dc-8ad82d31028c
</code></pre></td></tr></table></div></div><h2 id=how-to-list-rbd-and-pools-and-show-size-for-a-rbd-image>How to list rbd and pools, and show size for a rbd image?</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sh data-lang=sh>sudo ceph osd lspools
sudo rbd ls kube  <span class=c1># kube is the pool name</span>
subo rbd info kube/&lt;image-id&gt;  <span class=c1># kube is the pool, followed by image id</span>
</code></pre></td></tr></table></div></div></div><footer class=post-footer><nav class=post-nav><a class=prev href=/post/tips/nginx/><i class="iconfont icon-left"></i><span class="prev-text nav-default">nginx</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/post/tips/evernote/><span class="next-text nav-default">evernote</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:leowa@outlook.com class="iconfont icon-email" title=email></a><a href=https://github.com/leowa class="iconfont icon-github" title=github></a><a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=copyright-year>&copy;
2019 -
2020
<span class=heart><i class="iconfont icon-heart"></i></span><span class=author>Andy Zhang</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/dist/even.26188efa.min.js></script></body></html>